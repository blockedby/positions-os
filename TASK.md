Принято. Твое замечание справедливо: для профессионального проекта «черновика» недостаточно. Нам нужна **система управления знаниями и источниками**, а не просто скрипт-парсер.

Ниже представлена детальная техническая спецификация (SDN — System Design Notice), описывающая схемы работы каждого модуля, структуру хранения источников и механизмы кэширования.

---

# Техническая Спецификация: Архитектура Job-Hunter OS

## 1. Схема управления источниками (Source Management)

Чтобы парсинг запускался «по кнопке», система должна знать, _куда_ идти. Мы вводим понятие **Scraping Target**.

### Хранение в БД (Таблица `scraping_targets`):

| Поле           | Тип       | Описание                                                                 |
| -------------- | --------- | ------------------------------------------------------------------------ |
| `id`           | UUID      | Уникальный ID источника                                                  |
| `name`         | String    | Понятное имя (например, "Go Middle HH.ru")                               |
| `type`         | Enum      | `HH_SEARCH`, `LINKEDIN_PAGE`, `TG_CHANNEL`, `TG_GROUP`                   |
| `url`          | String    | Полная ссылка на поиск или @username канала                              |
| `metadata`     | JSONB     | Специфичные настройки: селекторы, cookies (для браузера), лимиты страниц |
| `last_scraped` | Timestamp | Время последнего запуска                                                 |
| `is_active`    | Boolean   | Включен ли источник в общий цикл                                         |

---

## 2. Детальные схемы работы сервисов

### 2.1. Collector Service (Интеллектуальный сбор)

Этот сервис не просто качает текст, он управляет состоянием «просмотренного».

1. **Триггер:** Сообщение из NATS `scrape.start(target_id)`.
2. **Загрузка контекста:** Сервис берет URL из `scraping_targets`.
3. **Исполнение (Strategy Pattern):**

- **TG:** Подключается через `gotgproto`, забирает последние N сообщений.
- **Browser:** Открывает Rod, эмулирует скролл, забирает HTML/JSON.

4. **Дедупликация (Кэш):**

- Для каждой вакансии вычисляется `content_hash` (SHA-256 от описания) и проверяется `external_id` (ID в HH или ID сообщения в TG).
- Если вакансия уже есть в БД в таблице `jobs` — **пропускаем**.

5. **Результат:** Сохранение в БД со статусом `RAW`.

### 2.2. Analyzer Service (Локальный AI-фильтр)

Работает на локальной LLM (например, Llama 3 via Ollama), чтобы не тратить деньги на мусор.

1. **Вход:** Список вакансий со статусом `RAW`.
2. **Промпт-анализ:** "Извлеки из текста: зарплату, город, удаленка (да/нет), язык (RU/EN), основные технологии".
3. **Обновление БД:**

- Запись структурированных данных в поля `salary_min`, `salary_max`, `currency`, `location`, `detected_language`.
- Статус меняется на `ANALYZED`.

4. **UI-статус:** Вакансия появляется в админке в списке «Новые».

### 2.3. Brain Service (Tailoring & PDF)

Запускается только после твоего одобрения конкретной вакансии в UI.

1. **Вход:** `job_id` + `base_resume_id`.
2. **Генерация:**

- **Resume:** LLM переписывает `Summary` и `Experience`, вставляя ключевые слова из вакансии.
- **Cover Letter:** Создает уникальный текст письма.

3. **Хранение:** В таблице `job_applications` сохраняется конкретная версия Markdown для этой вакансии. **Это важно: у каждой вакансии своя версия резюме.**
4. **Рендеринг:** Document Service преобразует этот Markdown в HTML, накладывает CSS и через `Rod.PrintToPDF` создает бинарный файл.

---

## 3. Схема Базы Данных (ER-диаграмма логики)

### Таблица `jobs` (Центральное хранилище)

- `id`: UUID
- `target_id`: FK к `scraping_targets`
- `external_id`: String (уникальный ID с сайта)
- `status`: (RAW, ANALYZED, REJECTED, INTERESTED, TAILORED, SENT)
- `raw_content`: Text
- `structured_data`: JSONB (вилка, локация, язык, стек)
- `created_at / updated_at`

### Таблица `job_applications` (Результаты работы AI)

- `id`: UUID
- `job_id`: FK к `jobs`
- `custom_resume_markdown`: Text (адаптированное резюме)
- `cover_letter`: Text
- `pdf_path`: String (путь к сгенерированному файлу в volume)
- `sent_at`: Timestamp
- `delivery_channel`: (TG_PRIVATE, TG_BOT, EMAIL)

---

## 4. Общий процесс (Sequence Diagram)

1. **Пользователь:** Нажимает "Спарсить HH" в Web UI.
2. **Web UI -> NATS:** Публикует `command.scrape {target: "HH_Go"}`.
3. **Collector:** Забирает вакансии -> Считает хеши -> Новые пишет в Postgres (`status: RAW`).
4. **Analyzer:** Видит новые `RAW`, прогоняет через локальную LLM -> Пишет метаданные (`status: ANALYZED`).
5. **Пользователь:** Открывает Dashboard, видит вакансию: "Go Dev, 5000$, Remote, EN". Нажимает **"Подготовить отклик"**.
6. **Brain Service:** Стучится в OpenAI API -> Генерит Markdown -> Document Service делает PDF. (`status: TAILORED`).
7. **Пользователь:** Читает текст, правит пару слов в UI, жмет **"Отправить в Telegram"**.
8. **Dispatcher:** Берет сессию `gotgproto`, шлет файл и текст рекрутеру. (`status: SENT`).

---

## 5. Технические детали реализации "Кэша"

Для исключения повторной обработки мы используем **Bloom Filter** (опционально) или просто **Unique Index** в PostgreSQL по связке `(target_id, external_id)`.

- Если парсер видит, что `external_id` уже существует — он обновляет дату `last_seen`, но не создает новую запись и не запускает AI заново.
- Это экономит токены и время.

---

## 6. Требования к UI (Детально)

- **Страница Конфигурации:** Список всех `scraping_targets` с возможностью добавить новый URL или TG канал.
- **Лог событий:** Лента в реальном времени (через WebSockets), показывающая: "Парсер запущен", "Найдено 10 новых вакансий", "AI закончил анализ".
- **Редактор:** Встроенный Markdown-редактор для финальной правки резюме перед генерацией PDF.

---
